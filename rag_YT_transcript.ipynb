{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d9efea",
   "metadata": {},
   "source": [
    "# Building a RAG application from scratch using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a540c3d",
   "metadata": {},
   "source": [
    "# Setting up the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a96bcb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# This is the YouTube video we're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c64044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f2c5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the model is working\n",
    "# model.invoke(\"who won 2011 cricket world cup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a80e5c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Australia won the 2015 Cricket World Cup by defeating New Zealand in the final.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result from the model is an AIMessage instance containing the answer.\n",
    "# We can extract this answer by chaining the model with an output parser.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"who won 2015 cricket world cup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7113558",
   "metadata": {},
   "source": [
    "# Introducing prompt templates\n",
    "\n",
    "Need to provide the model with some context and the question. Prompt templates are a simple way to define and reuse prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67f9d2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac4e07df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Susana'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Mary's sister is Susana\",\n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "394369c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {answer} to {language}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "372fbfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MarÃ­a tiene una hermana, Susana.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Mary have?\",\n",
    "        \"language\": \"Spanish\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550831b6",
   "metadata": {},
   "source": [
    "# Transcribing the YouTube Video\n",
    "\n",
    "I had issues downloading ffmpeg on the computer so the below script didnot work. Downloaded the txt file separatley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7129c4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f84949ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tempfile\n",
    "# import whisper\n",
    "# from pytube import YouTube\n",
    "\n",
    "\n",
    "# # Let's do this only if we haven't created the transcription file yet.\n",
    "# if not os.path.exists(\"transcription.txt\"):\n",
    "#     youtube = YouTube(YOUTUBE_VIDEO)\n",
    "#     audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "#     # Let's load the base model. This is not the most accurate\n",
    "#     # model but it's fast.\n",
    "#     whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "#     with tempfile.TemporaryDirectory() as tmpdir:\n",
    "#         file = audio.download(output_path=tmpdir, filename=\"audio.mp4\")\n",
    "#         transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "#         with open(\"transcription.txt\", \"w\") as transcription_file:\n",
    "#             transcription_file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92e673f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfb1aa",
   "metadata": {},
   "source": [
    "# Using the entire transcription as context\n",
    "If we try to invoke the chain using the transcription as context, the model will return an error because the context is too long.\n",
    "\n",
    "Large Language Models support limitted context sizes. The video we are using is too long for the model to handle, so we need to find a different solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2529e6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 47047 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"Is reading papers a good idea?\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abb68d",
   "metadata": {},
   "source": [
    "# Splitting the transcription\n",
    "Since we can't use the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3c2e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by loading the transcription in memory:\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "# text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1014d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ffd18",
   "metadata": {},
   "source": [
    "# Finding the relevant chunks\n",
    "Given a particular question, we need to find the relevant chunks from the transcription to send to the model. Here is where the idea of embeddings comes into play.\n",
    "\n",
    "An embedding is a mathematical representation of the semantic meaning of a word, sentence, or document. It's a projection of a concept in a high-dimensional space. Embeddings have a simple characteristic: The projection of related concepts will be close to each other, while concepts with different meanings will lie far away. You can use the Cohere's Embed Playground to visualize embeddings in two dimensions.\n",
    "\n",
    "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54c03125",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docarray in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (0.40.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from docarray) (1.23.4)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from docarray) (3.10.3)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from docarray) (2.7.1)\n",
      "Requirement already satisfied: rich>=13.1.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from docarray) (13.7.1)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from docarray) (2.32.0.20240602)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from pydantic>=1.10.8->docarray) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from pydantic>=1.10.8->docarray) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from pydantic>=1.10.8->docarray) (4.10.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from rich>=13.1.0->docarray) (2.13.0)\n",
      "Requirement already satisfied: urllib3>=2 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from types-requests>=2.28.11.6->docarray) (2.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"langchain[docarray]\"\n",
    "!pip install -U docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ddf11763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[-0.0013594045786472944, -0.03437049808954927, -0.01142556447128598, 0.0012913952108823416, -0.02616560552048414, 0.009161713858426773, -0.015621817294155089, 0.001822962257550091, -0.01180078783066434, -0.03324482708009158]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# example of how embedding works\n",
    "embedded_query = embeddings.embed_query(\"Who is Mary's sister?\")\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ff977",
   "metadata": {},
   "source": [
    "# Setting up a Vector Store and Connecting the vector store to the chain\n",
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a vector store.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches.\n",
    "\n",
    "We can use the vector store to find the most relevant chunks from the transcription to send to the model. Here is how we can connect the vector store to the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66f8773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e8dbaa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Synthetic intelligence is described as the next stage of development that involves artificial intelligence systems that are capable of uncovering puzzles in the universe and solving them.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup = RunnableParallel(context=vectorstore2.as_retriever(), question=RunnablePassthrough())\n",
    "chain = setup | prompt | model | parser\n",
    "chain.invoke(\"What is synthetic intelligence?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d55297",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84836d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Synthetic intelligence is described as the next stage of development in the context provided. It is suggested that synthetic AIs will uncover the puzzle of the universe and solve it.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"What is synthetic intelligence?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5bda48",
   "metadata": {},
   "source": [
    "# Storing vector store on local disk and then using it to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c4a818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (1.8.0)\r\n",
      "Requirement already satisfied: numpy in /home/girishj/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from faiss-cpu) (1.23.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3bea681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2fe1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS vector store from documents\n",
    "vector_store = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c4b42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector store to local disk\n",
    "faiss_index_path = \"local_faiss_index\"\n",
    "vector_store.save_local(faiss_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "082a51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vector_store = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ed27d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hollywood is going to start using AI to generate scenes for movies.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": loaded_vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is Hollywood going to start doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e699c8",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b76fa937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Synthetic intelligence is described as the next stage of development in the context provided. It refers to artificial intelligence systems that are created artificially by humans.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup = RunnableParallel(context=loaded_vector_store.as_retriever(), question=RunnablePassthrough())\n",
    "chain = setup | prompt | model | parser\n",
    "chain.invoke(\"What is synthetic intelligence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a19348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
